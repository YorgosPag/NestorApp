/**
 * =============================================================================
 * ğŸ¢ ENTERPRISE: AI REPLY GENERATOR
 * =============================================================================
 *
 * Centralized utility for generating dynamic, context-aware email replies
 * using OpenAI. Used by UC modules to create natural-sounding replies instead
 * of static templates.
 *
 * Architecture:
 *   - Calls OpenAI Responses API directly (NOT via OpenAIAnalysisProvider)
 *   - Free-form text output (no JSON schema)
 *   - Non-fatal: always falls back to static template on failure
 *   - Reusable by any UC module (appointment, property search, etc.)
 *
 * @module services/ai-pipeline/shared/ai-reply-generator
 * @see ADR-080 (Pipeline Implementation)
 * @see ADR-169 (Modular AI Architecture)
 */

import 'server-only';

import { AI_ANALYSIS_DEFAULTS } from '@/config/ai-analysis-config';
import { PIPELINE_REPLY_CONFIG } from '@/config/ai-pipeline-config';
import { createModuleLogger } from '@/lib/telemetry/Logger';
import type { SenderHistoryEntry } from './sender-history';

const logger = createModuleLogger('ai-reply-generator');

// ============================================================================
// TYPES
// ============================================================================

/** Context for generating an AI reply â€” passed by the UC module */
export interface AIReplyContext {
  /** Use case identifier for prompt selection */
  useCase: 'appointment' | 'property_search' | 'general';
  /** Sender's name for greeting */
  senderName: string;
  /** Whether sender is a known CRM contact */
  isKnownContact: boolean;
  /** Original email body (will be trimmed to MAX_ORIGINAL_MESSAGE_CHARS) */
  originalMessage: string;
  /** Original email subject */
  originalSubject: string;
  /** Module-specific context â€” injected into the prompt */
  moduleContext: Record<string, string | null>;
  /** Previous emails from same sender (privacy-safe: subject + date + intent only) */
  senderHistory?: SenderHistoryEntry[];
  /** Whether this sender has contacted before */
  isReturningContact?: boolean;
}

/** Result from the AI reply generation */
export interface AIReplyResult {
  /** The generated reply text */
  replyText: string;
  /** Whether AI generation was used (false = static fallback) */
  aiGenerated: boolean;
  /** Model used for generation (null if fallback) */
  model: string | null;
  /** Generation time in ms */
  durationMs: number;
}

// ============================================================================
// SYSTEM PROMPTS â€” per use case
// ============================================================================

const SYSTEM_PROMPTS: Record<AIReplyContext['useCase'], string> = {
  appointment: `Î•Î¯ÏƒÎ±Î¹ Î²Î¿Î·Î¸ÏŒÏ‚ ÎºÏ„Î·Î¼Î±Ï„Î¿Î¼ÎµÏƒÎ¹Ï„Î¹ÎºÎ¿Ï/ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î±ÏƒÏ„Î¹ÎºÎ¿Ï Î³ÏÎ±Ï†ÎµÎ¯Î¿Ï… ÏƒÏ„Î·Î½ Î•Î»Î»Î¬Î´Î±.
Î“ÏÎ¬ÏˆÎµ Î•Î Î‘Î“Î“Î•Î›ÎœÎ‘Î¤Î™ÎšÎŸ email Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·Ï‚ ÏƒÏ„Î± ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬ ÏƒÎµ Ï€ÎµÎ»Î¬Ï„Î· Ï€Î¿Ï… Î¶Î®Ï„Î·ÏƒÎµ ÏÎ±Î½Ï„ÎµÎ²Î¿Ï.

ÎšÎ‘ÎÎŸÎÎ•Î£:
1. Î¤ÏŒÎ½Î¿Ï‚: Î•Ï…Î³ÎµÎ½Î¹ÎºÏŒÏ‚, ÎµÏ€Î±Î³Î³ÎµÎ»Î¼Î±Ï„Î¹ÎºÏŒÏ‚, ÏƒÏÎ½Ï„Î¿Î¼Î¿Ï‚
2. Î“Î»ÏÏƒÏƒÎ±: Î•Î»Î»Î·Î½Î¹ÎºÎ¬ (Ï€Î»Î·Î¸Ï…Î½Ï„Î¹ÎºÏŒÏ‚ ÎµÏ…Î³ÎµÎ½ÎµÎ¯Î±Ï‚ â€” ÎµÏƒÎµÎ¯Ï‚/ÏƒÎ±Ï‚)
3. ÎœÎ¿ÏÏ†Î®: Plain text Î¼ÏŒÎ½Î¿ â€” Î§Î©Î¡Î™Î£ HTML tags, markdown, Î±ÏƒÏ„ÎµÏÎ¯ÏƒÎºÎ¿Ï…Ï‚ Î® formatting
4. ÎœÎ®ÎºÎ¿Ï‚: 5-10 Î³ÏÎ±Î¼Î¼Î­Ï‚ Î¼Î­Î³Î¹ÏƒÏ„Î¿
5. ÎÎµÎºÎ¯Î½Î± Ï€Î¬Î½Ï„Î± Î¼Îµ "Î‘Î³Î±Ï€Î·Ï„Î­/Î® [ÎŸÎ½Î¿Î¼Î±],"
6. Î¤Î­Î»ÎµÎ¹Ï‰ÏƒÎµ Ï€Î¬Î½Ï„Î± Î¼Îµ "ÎœÎµ ÎµÎºÏ„Î¯Î¼Î·ÏƒÎ·," â€” Î§Î©Î¡Î™Î£ Ï…Ï€Î¿Î³ÏÎ±Ï†Î® Î¼ÎµÏ„Î¬ (Ï€ÏÎ¿ÏƒÏ„Î¯Î¸ÎµÏ„Î±Î¹ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î±)
7. ÎœÎ—Î Ï…Ï€Î¿ÏƒÏ‡ÎµÎ¸ÎµÎ¯Ï‚ Ï€ÏÎ¬Î³Î¼Î±Ï„Î± Ï€Î¿Ï… Î´ÎµÎ½ Î³Î½Ï‰ÏÎ¯Î¶ÎµÎ¹Ï‚
8. ÎœÎ—Î Î±Î½Î±Ï†Î­ÏÎµÎ¹Ï‚ ÎµÏƒÏ‰Ï„ÎµÏÎ¹ÎºÎ­Ï‚ Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯ÎµÏ‚ Î® AI
9. Î‘Î½Î±Ï†Î­ÏÏƒÎ¿Ï… ÏƒÏ„Î¿ Î Î•Î¡Î™Î•Î§ÎŸÎœÎ•ÎÎŸ Ï„Î¿Ï… Î¼Î·Î½ÏÎ¼Î±Ï„Î¿Ï‚ Ï„Î¿Ï… Ï€ÎµÎ»Î¬Ï„Î· â€” Î¼Î·Î½ Î±Î³Î½Î¿ÎµÎ¯Ï‚ Ï„Î¹ Î­Î³ÏÎ±ÏˆÎµ
10. Î‘Î½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î±/ÏÏÎ±, ÎµÏ€Î¹Î²ÎµÎ²Î±Î¯Ï‰ÏƒÎ­ Ï„Î±
11. Î‘Î½ Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î±/ÏÏÎ±, Î±Î½Î­Ï†ÎµÏÎµ ÏŒÏ„Î¹ Î¸Î± ÎµÏ€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î®ÏƒÎµÏ„Îµ ÏƒÏÎ½Ï„Î¿Î¼Î± Î³Î¹Î± ÎºÎ±Î¸Î¿ÏÎ¹ÏƒÎ¼ÏŒ`,

  property_search: `Î•Î¯ÏƒÎ±Î¹ Î²Î¿Î·Î¸ÏŒÏ‚ ÎºÏ„Î·Î¼Î±Ï„Î¿Î¼ÎµÏƒÎ¹Ï„Î¹ÎºÎ¿Ï/ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î±ÏƒÏ„Î¹ÎºÎ¿Ï Î³ÏÎ±Ï†ÎµÎ¯Î¿Ï… ÏƒÏ„Î·Î½ Î•Î»Î»Î¬Î´Î±.
Î“ÏÎ¬ÏˆÎµ Î•Î Î‘Î“Î“Î•Î›ÎœÎ‘Î¤Î™ÎšÎŸ email Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·Ï‚ ÏƒÏ„Î± ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬ ÏƒÎµ Ï€ÎµÎ»Î¬Ï„Î· Ï€Î¿Ï… Î±Î½Î±Î¶Î·Ï„Î¬ Î±ÎºÎ¯Î½Î·Ï„Î¿.

ÎšÎ‘ÎÎŸÎÎ•Î£:
1. Î¤ÏŒÎ½Î¿Ï‚: Î•Ï…Î³ÎµÎ½Î¹ÎºÏŒÏ‚, ÎµÏ€Î±Î³Î³ÎµÎ»Î¼Î±Ï„Î¹ÎºÏŒÏ‚, ÏƒÏÎ½Ï„Î¿Î¼Î¿Ï‚
2. Î“Î»ÏÏƒÏƒÎ±: Î•Î»Î»Î·Î½Î¹ÎºÎ¬ (Ï€Î»Î·Î¸Ï…Î½Ï„Î¹ÎºÏŒÏ‚ ÎµÏ…Î³ÎµÎ½ÎµÎ¯Î±Ï‚ â€” ÎµÏƒÎµÎ¯Ï‚/ÏƒÎ±Ï‚)
3. ÎœÎ¿ÏÏ†Î®: Plain text Î¼ÏŒÎ½Î¿ â€” Î§Î©Î¡Î™Î£ HTML tags, markdown, Î±ÏƒÏ„ÎµÏÎ¯ÏƒÎºÎ¿Ï…Ï‚ Î® formatting
4. ÎœÎ®ÎºÎ¿Ï‚: 5-12 Î³ÏÎ±Î¼Î¼Î­Ï‚ Î¼Î­Î³Î¹ÏƒÏ„Î¿
5. ÎÎµÎºÎ¯Î½Î± Ï€Î¬Î½Ï„Î± Î¼Îµ "Î‘Î³Î±Ï€Î·Ï„Î­/Î® [ÎŸÎ½Î¿Î¼Î±],"
6. Î¤Î­Î»ÎµÎ¹Ï‰ÏƒÎµ Ï€Î¬Î½Ï„Î± Î¼Îµ "ÎœÎµ ÎµÎºÏ„Î¯Î¼Î·ÏƒÎ·," â€” Î§Î©Î¡Î™Î£ Ï…Ï€Î¿Î³ÏÎ±Ï†Î® Î¼ÎµÏ„Î¬
7. ÎœÎ—Î Ï…Ï€Î¿ÏƒÏ‡ÎµÎ¸ÎµÎ¯Ï‚ Ï€ÏÎ¬Î³Î¼Î±Ï„Î± Ï€Î¿Ï… Î´ÎµÎ½ Î³Î½Ï‰ÏÎ¯Î¶ÎµÎ¹Ï‚
8. ÎœÎ—Î Î±Î½Î±Ï†Î­ÏÎµÎ¹Ï‚ ÎµÏƒÏ‰Ï„ÎµÏÎ¹ÎºÎ­Ï‚ Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯ÎµÏ‚ Î® AI
9. Î‘Î½Î±Ï†Î­ÏÏƒÎ¿Ï… ÏƒÏ„Î± ÎºÏÎ¹Ï„Î®ÏÎ¹Î± Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ·Ï‚ Ï„Î¿Ï… Ï€ÎµÎ»Î¬Ï„Î·`,

  general: `Î•Î¯ÏƒÎ±Î¹ Î²Î¿Î·Î¸ÏŒÏ‚ ÎºÏ„Î·Î¼Î±Ï„Î¿Î¼ÎµÏƒÎ¹Ï„Î¹ÎºÎ¿Ï/ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î±ÏƒÏ„Î¹ÎºÎ¿Ï Î³ÏÎ±Ï†ÎµÎ¯Î¿Ï… ÏƒÏ„Î·Î½ Î•Î»Î»Î¬Î´Î±.
Î“ÏÎ¬ÏˆÎµ Î•Î Î‘Î“Î“Î•Î›ÎœÎ‘Î¤Î™ÎšÎŸ email Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·Ï‚ ÏƒÏ„Î± ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬.

ÎšÎ‘ÎÎŸÎÎ•Î£:
1. Î¤ÏŒÎ½Î¿Ï‚: Î•Ï…Î³ÎµÎ½Î¹ÎºÏŒÏ‚, ÎµÏ€Î±Î³Î³ÎµÎ»Î¼Î±Ï„Î¹ÎºÏŒÏ‚, ÏƒÏÎ½Ï„Î¿Î¼Î¿Ï‚
2. Î“Î»ÏÏƒÏƒÎ±: Î•Î»Î»Î·Î½Î¹ÎºÎ¬ (Ï€Î»Î·Î¸Ï…Î½Ï„Î¹ÎºÏŒÏ‚ ÎµÏ…Î³ÎµÎ½ÎµÎ¯Î±Ï‚ â€” ÎµÏƒÎµÎ¯Ï‚/ÏƒÎ±Ï‚)
3. ÎœÎ¿ÏÏ†Î®: Plain text Î¼ÏŒÎ½Î¿ â€” Î§Î©Î¡Î™Î£ HTML tags, markdown, Î±ÏƒÏ„ÎµÏÎ¯ÏƒÎºÎ¿Ï…Ï‚ Î® formatting
4. ÎœÎ®ÎºÎ¿Ï‚: 5-8 Î³ÏÎ±Î¼Î¼Î­Ï‚ Î¼Î­Î³Î¹ÏƒÏ„Î¿
5. ÎÎµÎºÎ¯Î½Î± Ï€Î¬Î½Ï„Î± Î¼Îµ "Î‘Î³Î±Ï€Î·Ï„Î­/Î® [ÎŸÎ½Î¿Î¼Î±],"
6. Î¤Î­Î»ÎµÎ¹Ï‰ÏƒÎµ Ï€Î¬Î½Ï„Î± Î¼Îµ "ÎœÎµ ÎµÎºÏ„Î¯Î¼Î·ÏƒÎ·," â€” Î§Î©Î¡Î™Î£ Ï…Ï€Î¿Î³ÏÎ±Ï†Î® Î¼ÎµÏ„Î¬
7. ÎœÎ—Î Ï…Ï€Î¿ÏƒÏ‡ÎµÎ¸ÎµÎ¯Ï‚ Ï€ÏÎ¬Î³Î¼Î±Ï„Î± Ï€Î¿Ï… Î´ÎµÎ½ Î³Î½Ï‰ÏÎ¯Î¶ÎµÎ¹Ï‚
8. ÎœÎ—Î Î±Î½Î±Ï†Î­ÏÎµÎ¹Ï‚ ÎµÏƒÏ‰Ï„ÎµÏÎ¹ÎºÎ­Ï‚ Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯ÎµÏ‚ Î® AI`,
};

// ============================================================================
// INTERNAL: Prompt builders
// ============================================================================

function buildUserPrompt(context: AIReplyContext): string {
  const { senderName, originalSubject, originalMessage, moduleContext, senderHistory, isReturningContact } = context;

  const trimmedMessage = originalMessage.slice(0, PIPELINE_REPLY_CONFIG.MAX_ORIGINAL_MESSAGE_CHARS);

  // Build module-specific context lines
  const contextLines: string[] = [];
  for (const [key, value] of Object.entries(moduleContext)) {
    if (value !== null) {
      contextLines.push(`- ${key}: ${value}`);
    }
  }

  const contextBlock = contextLines.length > 0
    ? `\nÎ Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚:\n${contextLines.join('\n')}`
    : '';

  // Build sender history block (if available)
  let historyBlock = '';
  if (isReturningContact && senderHistory && senderHistory.length > 0) {
    const historyLines = senderHistory.map((entry) => {
      const dateFormatted = entry.date.slice(0, 10); // YYYY-MM-DD
      const intentLabel = entry.intent ? `, ${entry.intent}` : '';
      return `  - "${entry.subject}" (${dateFormatted}${intentLabel})`;
    });

    historyBlock = `\nÎ™ÏƒÏ„Î¿ÏÎ¹ÎºÏŒ Î±Ï€Î¿ÏƒÏ„Î¿Î»Î­Î± (Î¿ Ï€ÎµÎ»Î¬Ï„Î·Ï‚ Î­Ï‡ÎµÎ¹ ÏƒÏ„ÎµÎ¯Î»ÎµÎ¹ ${senderHistory.length} Ï€ÏÎ¿Î·Î³Î¿ÏÎ¼ÎµÎ½Î± emails):\n${historyLines.join('\n')}`;
  }

  return `ÎŸ Ï€ÎµÎ»Î¬Ï„Î·Ï‚ ${senderName} Î­ÏƒÏ„ÎµÎ¹Î»Îµ:
Î˜Î­Î¼Î±: ${originalSubject || '(Ï‡Ï‰ÏÎ¯Ï‚ Î¸Î­Î¼Î±)'}
ÎœÎ®Î½Ï…Î¼Î±: ${trimmedMessage || '(ÎºÎµÎ½ÏŒ Î¼Î®Î½Ï…Î¼Î±)'}
${contextBlock}${historyBlock}

Î“ÏÎ¬ÏˆÎµ Ï„Î·Î½ Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·.`;
}

// ============================================================================
// INTERNAL: OpenAI Responses API call (free-form text, no JSON schema)
// ============================================================================

/** Type guard for record objects */
function isRecord(value: unknown): value is Record<string, unknown> {
  return typeof value === 'object' && value !== null;
}

/**
 * Extract output text from OpenAI Responses API payload.
 * Mirrors the pattern from OpenAIAnalysisProvider (lines 74-101).
 */
function extractOutputText(payload: unknown): string | null {
  if (!isRecord(payload)) return null;

  // Direct output_text field (shortcut in newer API versions)
  const outputText = payload.output_text;
  if (typeof outputText === 'string' && outputText.trim().length > 0) {
    return outputText.trim();
  }

  // Walk the output array for message content
  const output = payload.output;
  if (!Array.isArray(output)) return null;

  for (const item of output) {
    if (!isRecord(item)) continue;
    if (item.type !== 'message') continue;
    const content = item.content;
    if (!Array.isArray(content)) continue;

    for (const entry of content) {
      if (!isRecord(entry)) continue;
      if (entry.type !== 'output_text') continue;
      const text = entry.text;
      if (typeof text === 'string' && text.trim().length > 0) {
        return text.trim();
      }
    }
  }

  return null;
}

interface OpenAIErrorPayload {
  error?: {
    message?: string;
    type?: string;
  };
}

/**
 * Call OpenAI Responses API for free-form text generation.
 * No JSON schema â€” just system prompt + user prompt â†’ plain text reply.
 */
async function callOpenAI(
  systemPrompt: string,
  userPrompt: string,
  requestId: string,
): Promise<string | null> {
  const apiKey = process.env.OPENAI_API_KEY;
  if (!apiKey) {
    logger.warn('OPENAI_API_KEY not set â€” skipping AI reply generation', { requestId });
    return null;
  }

  const baseUrl = AI_ANALYSIS_DEFAULTS.OPENAI.BASE_URL;
  const model = AI_ANALYSIS_DEFAULTS.OPENAI.TEXT_MODEL;
  const timeoutMs = PIPELINE_REPLY_CONFIG.TIMEOUT_MS;
  const maxRetries = PIPELINE_REPLY_CONFIG.MAX_RETRIES;

  const requestBody = {
    model,
    input: [
      {
        role: 'system',
        content: [{ type: 'input_text', text: systemPrompt }],
      },
      {
        role: 'user',
        content: [{ type: 'input_text', text: userPrompt }],
      },
    ],
    // No text.format â€” we want free-form text output, NOT JSON
  };

  let attempt = 0;

  while (attempt <= maxRetries) {
    try {
      const controller = new AbortController();
      const timeout = setTimeout(() => controller.abort(), timeoutMs);

      const response = await fetch(`${baseUrl}/responses`, {
        method: 'POST',
        headers: {
          Authorization: `Bearer ${apiKey}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody),
        signal: controller.signal,
      });

      clearTimeout(timeout);

      if (!response.ok) {
        const errorPayload = (await response.json().catch(() => ({}))) as OpenAIErrorPayload;
        const message = errorPayload.error?.message || `OpenAI error (${response.status})`;
        throw new Error(message);
      }

      const payload: unknown = await response.json();
      return extractOutputText(payload);
    } catch (error) {
      if (attempt >= maxRetries) {
        logger.error('OpenAI reply generation failed after retries', {
          requestId,
          error: error instanceof Error ? error.message : String(error),
        });
        return null;
      }
      attempt += 1;
    }
  }

  return null;
}

// ============================================================================
// INTERNAL: Reply validation
// ============================================================================

/**
 * Basic sanity checks on the AI-generated reply.
 * Ensures it looks like a valid Greek professional email.
 */
function validateReply(text: string): boolean {
  // Must start with a greeting
  if (!text.includes('Î‘Î³Î±Ï€Î·Ï„') && !text.includes('Î±Î³Î±Ï€Î·Ï„')) {
    return false;
  }

  // Must not contain HTML tags
  if (/<[a-z/][^>]*>/i.test(text)) {
    return false;
  }

  // Must not contain markdown formatting
  if (/^#{1,6}\s/m.test(text) || /\*\*[^*]+\*\*/m.test(text)) {
    return false;
  }

  // Must not be too long
  if (text.length > PIPELINE_REPLY_CONFIG.MAX_REPLY_CHARS) {
    return false;
  }

  // Must have reasonable length (at least 50 chars)
  if (text.length < 50) {
    return false;
  }

  return true;
}

// ============================================================================
// COMPOSITE REPLY â€” Multi-Intent (ADR-131)
// ============================================================================

const COMPOSITE_REPLY_SYSTEM_PROMPT = `Î•Î¯ÏƒÎ±Î¹ Î²Î¿Î·Î¸ÏŒÏ‚ ÎºÏ„Î·Î¼Î±Ï„Î¿Î¼ÎµÏƒÎ¹Ï„Î¹ÎºÎ¿Ï/ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î±ÏƒÏ„Î¹ÎºÎ¿Ï Î³ÏÎ±Ï†ÎµÎ¯Î¿Ï… ÏƒÏ„Î·Î½ Î•Î»Î»Î¬Î´Î±.
Î£Î¿Ï… Î´Î¯Î½Î¿Î½Ï„Î±Î¹ Î ÎŸÎ›Î›Î‘Î Î›Î•Î£ Î¼ÎµÏÎ¹ÎºÎ­Ï‚ Î±Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚ (ÎºÎ¬Î¸Îµ Î¼Î¯Î± Î³Î¹Î± Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÏŒ Î¸Î­Î¼Î±) ÎºÎ±Î¹ Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Ï„Î¹Ï‚ Î£Î¥ÎÎ˜Î•Î£Î•Î™Î£ ÏƒÎµ ÎœÎ™Î‘ ÎµÎ½Î¹Î±Î¯Î±, Î¿Î»Î¿ÎºÎ»Î·ÏÏ‰Î¼Î­Î½Î· Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·.

ÎšÎ‘ÎÎŸÎÎ•Î£:
1. Î¤ÏŒÎ½Î¿Ï‚: Î•Ï…Î³ÎµÎ½Î¹ÎºÏŒÏ‚, ÎµÏ€Î±Î³Î³ÎµÎ»Î¼Î±Ï„Î¹ÎºÏŒÏ‚, ÏƒÏÎ½Ï„Î¿Î¼Î¿Ï‚
2. Î“Î»ÏÏƒÏƒÎ±: Î•Î»Î»Î·Î½Î¹ÎºÎ¬ (Ï€Î»Î·Î¸Ï…Î½Ï„Î¹ÎºÏŒÏ‚ ÎµÏ…Î³ÎµÎ½ÎµÎ¯Î±Ï‚ â€” ÎµÏƒÎµÎ¯Ï‚/ÏƒÎ±Ï‚)
3. ÎœÎ¿ÏÏ†Î®: Plain text Î¼ÏŒÎ½Î¿ â€” Î§Î©Î¡Î™Î£ HTML tags, markdown, Î±ÏƒÏ„ÎµÏÎ¯ÏƒÎºÎ¿Ï…Ï‚ Î® formatting
4. ÎœÎ®ÎºÎ¿Ï‚: 8-15 Î³ÏÎ±Î¼Î¼Î­Ï‚ Î¼Î­Î³Î¹ÏƒÏ„Î¿
5. ÎÎµÎºÎ¯Î½Î± Ï€Î¬Î½Ï„Î± Î¼Îµ "Î‘Î³Î±Ï€Î·Ï„Î­/Î® [ÎŸÎ½Î¿Î¼Î±],"
6. Î¤Î­Î»ÎµÎ¹Ï‰ÏƒÎµ Ï€Î¬Î½Ï„Î± Î¼Îµ "ÎœÎµ ÎµÎºÏ„Î¯Î¼Î·ÏƒÎ·," â€” Î§Î©Î¡Î™Î£ Ï…Ï€Î¿Î³ÏÎ±Ï†Î® Î¼ÎµÏ„Î¬
7. Î•ÎÎŸÎ ÎŸÎ™Î—Î£Î• Ï„Î± Î¸Î­Î¼Î±Ï„Î± ÏƒÎµ Ï†Ï…ÏƒÎ¹ÎºÎ® ÏÎ¿Î® â€” ÎŸÎ§Î™ Î±ÏÎ¹Î¸Î¼Î·Î¼Î­Î½Î· Î»Î¯ÏƒÏ„Î±
8. Î‘Î½Î±Ï†Î­ÏÏƒÎ¿Ï… ÏƒÎµ ÎŸÎ›Î‘ Ï„Î± Î¸Î­Î¼Î±Ï„Î± â€” Î¼Î·Î½ Î±Î³Î½Î¿Î®ÏƒÎµÎ¹Ï‚ ÎºÎ±Î½Î­Î½Î±
9. ÎœÎ—Î Î±Î½Î±Ï†Î­ÏÎµÎ¹Ï‚ ÎµÏƒÏ‰Ï„ÎµÏÎ¹ÎºÎ­Ï‚ Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯ÎµÏ‚ Î® AI`;

/** Input for composite reply generation */
export interface CompositeReplyInput {
  /** Draft replies from individual UC modules */
  moduleReplies: Array<{
    useCase: string;
    draftReply: string;
  }>;
  /** Sender name for greeting */
  senderName: string;
  /** Original email body */
  originalMessage: string;
  /** Original email subject */
  originalSubject: string;
}

function buildCompositeUserPrompt(input: CompositeReplyInput): string {
  const trimmedMessage = input.originalMessage.slice(0, PIPELINE_REPLY_CONFIG.MAX_ORIGINAL_MESSAGE_CHARS);

  const repliesBlock = input.moduleReplies
    .map((r, i) => `--- ÎœÎµÏÎ¹ÎºÎ® Î‘Ï€Î¬Î½Ï„Î·ÏƒÎ· ${i + 1} (${r.useCase}) ---\n${r.draftReply}`)
    .join('\n\n');

  return `ÎŸ Ï€ÎµÎ»Î¬Ï„Î·Ï‚ ${input.senderName} Î­ÏƒÏ„ÎµÎ¹Î»Îµ:
Î˜Î­Î¼Î±: ${input.originalSubject || '(Ï‡Ï‰ÏÎ¯Ï‚ Î¸Î­Î¼Î±)'}
ÎœÎ®Î½Ï…Î¼Î±: ${trimmedMessage || '(ÎºÎµÎ½ÏŒ Î¼Î®Î½Ï…Î¼Î±)'}

ÎœÎµÏÎ¹ÎºÎ­Ï‚ Î±Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚ Ï€ÏÎ¿Ï‚ ÏƒÏÎ½Î¸ÎµÏƒÎ·:
${repliesBlock}

Î£ÏÎ½Î¸ÎµÏƒÎµ ÎœÎ™Î‘ ÎµÎ½Î¹Î±Î¯Î± Î±Ï€Î¬Î½Ï„Î·ÏƒÎ· Ï€Î¿Ï… ÎºÎ±Î»ÏÏ€Ï„ÎµÎ¹ ÎŸÎ›Î‘ Ï„Î± Î¸Î­Î¼Î±Ï„Î±.`;
}

/**
 * Generate a composite reply by merging multiple module-specific draft replies
 * into ONE unified, natural-sounding email.
 *
 * Single-reply shortcut: if only 1 module reply, returns it directly (zero overhead).
 *
 * @see ADR-131 (Multi-Intent Pipeline)
 */
export async function generateCompositeReply(
  input: CompositeReplyInput,
  requestId: string,
): Promise<AIReplyResult> {
  // Single reply â€” no composition needed
  if (input.moduleReplies.length <= 1) {
    return {
      replyText: input.moduleReplies[0]?.draftReply ?? '',
      aiGenerated: input.moduleReplies.length > 0,
      model: null,
      durationMs: 0,
    };
  }

  const startTime = Date.now();

  try {
    const userPrompt = buildCompositeUserPrompt(input);

    logger.info('Composite reply generation started', {
      requestId,
      moduleCount: input.moduleReplies.length,
      useCases: input.moduleReplies.map(r => r.useCase).join(','),
    });

    const replyText = await callOpenAI(COMPOSITE_REPLY_SYSTEM_PROMPT, userPrompt, requestId);
    const durationMs = Date.now() - startTime;

    if (!replyText || !validateReply(replyText)) {
      logger.warn('Composite reply failed validation â€” using concatenation fallback', {
        requestId,
        durationMs,
      });
      // Fallback: join individual replies with separator
      const fallbackText = input.moduleReplies.map(r => r.draftReply).join('\n\n');
      return {
        replyText: fallbackText,
        aiGenerated: false,
        model: null,
        durationMs,
      };
    }

    logger.info('Composite reply generation succeeded', {
      requestId,
      durationMs,
      model: AI_ANALYSIS_DEFAULTS.OPENAI.TEXT_MODEL,
      replyLength: replyText.length,
    });

    return {
      replyText,
      aiGenerated: true,
      model: AI_ANALYSIS_DEFAULTS.OPENAI.TEXT_MODEL,
      durationMs,
    };
  } catch (error) {
    const durationMs = Date.now() - startTime;
    logger.error('Composite reply unexpected error â€” using concatenation fallback', {
      requestId,
      error: error instanceof Error ? error.message : String(error),
      durationMs,
    });

    const fallbackText = input.moduleReplies.map(r => r.draftReply).join('\n\n');
    return {
      replyText: fallbackText,
      aiGenerated: false,
      model: null,
      durationMs,
    };
  }
}

// ============================================================================
// MAIN EXPORT
// ============================================================================

/**
 * Generate a dynamic AI reply for a customer email.
 *
 * Calls OpenAI to produce a natural, context-aware reply in Greek.
 * If ANYTHING fails (API error, timeout, bad output), falls back to
 * the provided static template function.
 *
 * @param context â€” Use case, sender info, original message, module-specific data
 * @param fallbackFn â€” Static template function (e.g. buildAppointmentReply)
 * @param requestId â€” Pipeline request ID for logging/correlation
 */
export async function generateAIReply(
  context: AIReplyContext,
  fallbackFn: () => string,
  requestId: string,
): Promise<AIReplyResult> {
  const startTime = Date.now();

  try {
    const systemPrompt = SYSTEM_PROMPTS[context.useCase];
    const userPrompt = buildUserPrompt(context);

    logger.info('AI reply generation started', {
      requestId,
      useCase: context.useCase,
      senderName: context.senderName,
    });

    const replyText = await callOpenAI(systemPrompt, userPrompt, requestId);
    const durationMs = Date.now() - startTime;

    if (!replyText) {
      logger.warn('AI reply generation returned empty â€” using fallback', { requestId, durationMs });
      return {
        replyText: fallbackFn(),
        aiGenerated: false,
        model: null,
        durationMs,
      };
    }

    // Validate the generated reply
    if (!validateReply(replyText)) {
      logger.warn('AI reply failed validation â€” using fallback', {
        requestId,
        durationMs,
        replyLength: replyText.length,
      });
      return {
        replyText: fallbackFn(),
        aiGenerated: false,
        model: null,
        durationMs,
      };
    }

    logger.info('AI reply generation succeeded', {
      requestId,
      durationMs,
      model: AI_ANALYSIS_DEFAULTS.OPENAI.TEXT_MODEL,
      replyLength: replyText.length,
    });

    return {
      replyText,
      aiGenerated: true,
      model: AI_ANALYSIS_DEFAULTS.OPENAI.TEXT_MODEL,
      durationMs,
    };
  } catch (error) {
    const durationMs = Date.now() - startTime;
    logger.error('AI reply generation unexpected error â€” using fallback', {
      requestId,
      error: error instanceof Error ? error.message : String(error),
      durationMs,
    });

    return {
      replyText: fallbackFn(),
      aiGenerated: false,
      model: null,
      durationMs,
    };
  }
}
